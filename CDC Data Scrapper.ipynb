{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd \n",
    "import requests\n",
    "import datetime as datetime\n",
    "import time\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.action_chains import ActionChains \n",
    "\n",
    "option = Options()\n",
    "option.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options = option)\n",
    "#actions = ActionChains(driver)\n",
    "#link equal to daily covid data \n",
    "link = \"https://covid.cdc.gov/covid-data-tracker/#trends_totalandratecases\"\n",
    "driver.get(link)\n",
    "#let driver get link, takes time to load everything in\n",
    "time.sleep(10)\n",
    "#click on drop down menu and first option in drop down menu for states \n",
    "driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/i\").click()\n",
    "driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/div[2]/div[2]\").click()\n",
    "#parse through the html\n",
    "source = driver.page_source \n",
    "#information needed is embedded within several tags so we go through each tag to get the info we want\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "soup_2 =  BeautifulSoup(source, 'html.parser')\n",
    "info = soup.find_all('table', class_= 'expanded-data-table')\n",
    "info_2 = soup.find_all('tr')\n",
    "info_3 = soup.find_all('td')\n",
    "info_3 = list(info_3)\n",
    "\n",
    "#scrape the list of dates available in the data\n",
    "for i in range(0,len(info_3)):\n",
    "    info_3[i] = info_3[i]['aria-label']\n",
    "date = []\n",
    "for i in range(0,len(info_3),3):\n",
    "    holder = info_3[i].split(' ')\n",
    "    date.append(holder[1]+' ' + holder[2]+ ' '+ holder[3])\n",
    "\n",
    "#create dataframe for everything scrapped starting with Date as the first column\n",
    "covid_data = list(zip(date))\n",
    "covid_data = pd.DataFrame(covid_data, columns = ['Date'])\n",
    "\n",
    "#scrape all the state names as the CDC data includes NYC as separate as well as territories\n",
    "state_names = []\n",
    "states = soup_2.find_all('div', class_ = 'item')\n",
    "for i in range(0,len(states)):\n",
    "    state_names.append(states[i].text)\n",
    "\n",
    "#chromedriver to go to the cdc website \n",
    "option = Options()\n",
    "option.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options = option)\n",
    "link = \"https://covid.cdc.gov/covid-data-tracker/#trends_totalandratecases\"\n",
    "driver.get(link)\n",
    "time.sleep(10)\n",
    "#click on the dropdown menu again\n",
    "driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/i\").click()\n",
    "time.sleep(5)\n",
    "#get total count of number of items in dropdown menu\n",
    "total_count = driver.find_elements_by_xpath(\"//div[@class='menu transition visible']/div\")\n",
    "#loop through drop down menu one by one\n",
    "#reason for this is that the table on the site is updated as you click through each option\n",
    "#each option is clicked, the data of interest is scrapped, and next item is clicked and so on\n",
    "for i in range(len(total_count)):\n",
    "\n",
    "    path = \"//div[@class='menu transition visible']/div[{}]\".format(i+1)\n",
    "\n",
    "    driver.find_element_by_xpath(path).click()\n",
    "    ####scrape here\n",
    "    source = driver.page_source \n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    info = soup.find_all('table', class_= 'expanded-data-table')\n",
    "    info_2 = soup.find_all('tr')\n",
    "    info_3 = soup.find_all('td')\n",
    "    info_3 = list(info_3)\n",
    "    #loop through to remove the aria-label in front of each piece of data we want\n",
    "    for x in range(0,len(info_3)):\n",
    "        info_3[x] = info_3[x]['aria-label']\n",
    "    daily_case = []\n",
    "    case_rate = []\n",
    "\n",
    "    #find the daily cases and well as cases per 100,000\n",
    "    for x in range(1, len(info_3),3):\n",
    "        holder = info_3[x].split(' ')\n",
    "        daily_case.append(holder[-1])\n",
    "\n",
    "    for x in range(2, len(info_3),3):\n",
    "        holder = info_3[x].split(' ')\n",
    "        case_rate.append(holder[-1])\n",
    "    #scrapped data added to overall dataframe including name of territory/state as well as what the data represents\n",
    "    \n",
    "    covid_data[state_names[i] + ' Total Cases'] = list(daily_case)\n",
    "    covid_data[state_names[i] + ' Cases Per 100,000'] = list(case_rate)\n",
    "    print(state_names[i]+' read in for cases')\n",
    "    #click drop down menu to reset \n",
    "    driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/i\").click()\n",
    "    time.sleep(3)\n",
    "    \n",
    "#below is the same process but for COVID death data whereas above was just for cases\n",
    "option = Options()\n",
    "option.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options = option)\n",
    "#actions = ActionChains(driver)\n",
    "#link equal to daily covid data \n",
    "link = \"https://covid.cdc.gov/covid-data-tracker/#trends_totalandratedeaths\"\n",
    "driver.get(link)\n",
    "#let driver get link, takes time to load everything in\n",
    "time.sleep(10)\n",
    "\n",
    "driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/i\").click()\n",
    "driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/div[2]/div[2]\").click()\n",
    "#parse through the html\n",
    "source = driver.page_source \n",
    "#information needed is embedded within several tags so we go through each tag to get the info we want\n",
    "soup = BeautifulSoup(source, 'html.parser')\n",
    "soup_2 =  BeautifulSoup(source, 'html.parser')\n",
    "info = soup.find_all('table', class_= 'expanded-data-table')\n",
    "info_2 = soup.find_all('tr')\n",
    "info_3 = soup.find_all('td')\n",
    "info_3 = list(info_3)\n",
    "\n",
    "for i in range(0,len(info_3)):\n",
    "    info_3[i] = info_3[i]['aria-label']\n",
    "date = []\n",
    "for i in range(0,len(info_3),3):\n",
    "    holder = info_3[i].split(' ')\n",
    "    date.append(holder[1]+' ' + holder[2]+ ' '+ holder[3])\n",
    "\n",
    "covid_data_death = list(zip(date))\n",
    "covid_data_death = pd.DataFrame(covid_data_death, columns = ['Date'])\n",
    "    \n",
    "state_names = []\n",
    "states = soup_2.find_all('div', class_ = 'item')\n",
    "for i in range(0,len(states)):\n",
    "    state_names.append(states[i].text)\n",
    "\n",
    "option = Options()\n",
    "option.add_argument('--headless')\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install(),options = option)\n",
    "link = \"https://covid.cdc.gov/covid-data-tracker/#trends_totalandratedeaths\"\n",
    "driver.get(link)\n",
    "time.sleep(10)\n",
    "driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/i\").click()\n",
    "time.sleep(5)\n",
    "total_count = driver.find_elements_by_xpath(\"//div[@class='menu transition visible']/div\")\n",
    "for i in range(len(total_count)):\n",
    "   \n",
    "    path = \"//div[@class='menu transition visible']/div[{}]\".format(i+1)\n",
    "\n",
    "    driver.find_element_by_xpath(path).click()\n",
    "    ####scrape here\n",
    "    source = driver.page_source \n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "    info = soup.find_all('table', class_= 'expanded-data-table')\n",
    "    info_2 = soup.find_all('tr')\n",
    "    info_3 = soup.find_all('td')\n",
    "    info_3 = list(info_3)\n",
    "    #loop through to remove the aria-label in front of each piece of data we want\n",
    "    for x in range(0,len(info_3)):\n",
    "        info_3[x] = info_3[x]['aria-label']\n",
    "    daily_case = []\n",
    "    case_rate = []\n",
    "\n",
    "   #get daily deaths as well as deaths per 100,000\n",
    "    for x in range(1, len(info_3),3):\n",
    "        holder = info_3[x].split(' ')\n",
    "        daily_case.append(holder[-1])\n",
    "\n",
    "    for x in range(2, len(info_3),3):\n",
    "        holder = info_3[x].split(' ')\n",
    "        case_rate.append(holder[-1])\n",
    "\n",
    "    covid_data_death[state_names[i] + ' Total Deaths'] = list(daily_case)\n",
    "    covid_data_death[state_names[i] + ' Deaths Per 100,000'] = list(case_rate)\n",
    "    print(state_names[i]+' read in for deaths')\n",
    "    driver.find_element_by_xpath(\"/html/body/main/div/div[1]/div/div/div[1]/div[1]/div/div[1]/div[2]/i\").click()\n",
    "    time.sleep(3)\n",
    "covid_data.to_csv('covid_state_cases.csv')\n",
    "covid_data_death.to_csv('covid_state_deaths.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
